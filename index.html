<!-- DIGITAL CITIZENSHIP GUIDE: UNDERSTANDING AI ALGORITHMIC BIAS -->
<!-- Paste this whole block into Google Sites: Insert → Embed → Embed code -->
<section id="ai-bias-guide" class="dcg-root" aria-labelledby="dcg-title">
    <style>
        /* ---------- Base tokens ---------- */
        .dcg-root {
            --bg: #ffffff;
            --fg: #0f172a;           /* near-slate-900 */
            --muted: #475569;        /* slate-600 */
            --link: #0a66c2;         /* accessible blue */
            --accent: #2563eb;       /* blue-600 */
            --accent-2: #22c55e;     /* green-500 */
            --warn: #b45309;         /* amber-700 */
            --card: #f8fafc;         /* slate-50 */
            --border: #e2e8f0;       /* slate-200 */
            --shadow: 0 8px 28px rgba(2, 6, 23, 0.08);
            font-family: system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, "Apple Color Emoji", "Segoe UI Emoji";
            color: var(--fg);
            background: var(--bg);
            line-height: 1.55;
            box-sizing: border-box;
            max-width: 1200px;
            margin: 0 auto;
            padding: 1rem;
        }

        /* ---------- Layout ---------- */
        .dcg-wrap {
            display: grid;
            grid-template-columns: 280px 1fr;
            gap: 1.5rem;
        }
        @media (max-width: 1024px) {
            .dcg-wrap { grid-template-columns: 1fr; }
            .dcg-toc { position: static; top: auto; }
        }

        /* ---------- Header ---------- */
        .dcg-header {
            margin: 0 0 1rem 0;
            padding: 1rem 1.25rem;
            border: 1px solid var(--border);
            border-radius: 16px;
            background: linear-gradient(180deg, #f8fafc, #ffffff);
            box-shadow: var(--shadow);
        }
        .dcg-title {
            font-size: clamp(1.5rem, 2.4vw, 2rem);
            margin: 0 0 .25rem 0;
        }
        .dcg-sub {
            margin: 0;
            color: var(--muted);
            font-size: .975rem;
        }

        /* ---------- TOC ---------- */
        .dcg-toc {
            position: sticky;
            top: 12px;
            height: fit-content;
            border: 1px solid var(--border);
            border-radius: 14px;
            padding: .75rem;
            background: var(--card);
        }
        .dcg-toc h3 {
            font-size: 1rem;
            margin: .25rem .5rem .25rem;
            color: var(--muted);
        }
        .dcg-toc a {
            display: block;
            padding: .5rem .75rem;
            margin: .125rem 0;
            border-radius: 10px;
            color: var(--fg);
            text-decoration: none;
            border: 1px solid transparent;
        }
        .dcg-toc a:hover,
        .dcg-toc a[aria-current="true"] {
            background: #eef2ff;
            border-color: #c7d2fe;
            color: #1e40af;
        }

        /* ---------- Content ---------- */
        .dcg-content {
            display: grid;
            gap: 1rem;
        }
        .dcg-section {
            border: 1px solid var(--border);
            border-radius: 16px;
            background: var(--bg);
            box-shadow: var(--shadow);
            overflow: hidden;
        }
        .dcg-sec-head {
            display: flex;
            align-items: center;
            justify-content: space-between;
            gap: 1rem;
            padding: 1rem 1.25rem;
            background: #f1f5f9;
            border-bottom: 1px solid var(--border);
        }
        .dcg-sec-head h2,
        .dcg-sec-head h3 {
            margin: 0;
            font-size: clamp(1.1rem, 1.5vw, 1.25rem);
            color: #003366;
        }
        .dcg-sec-body {
            padding: 1rem 1.25rem 1.25rem;
        }
        .dcg-sec-body h3 {
            color: #003366;
            margin-top: 1rem;
        }
        .dcg-muted { color: var(--muted); }

        /* ---------- Callouts ---------- */
        .dcg-callout {
            border: 1px solid var(--border);
            background: #f8fafc;
            border-radius: 12px;
            padding: .875rem 1rem;
            display: grid;
            gap: .25rem;
        }
        .dcg-callout.good { border-left: 4px solid var(--accent-2); }
        .dcg-callout.info { border-left: 4px solid var(--accent); }
        .dcg-callout.warn { border-left: 4px solid var(--warn); }

        /* ---------- List styling ---------- */
        .dcg-content ul { padding-left: 1.2rem; }
        .dcg-content li { margin: .25rem 0; }

        /* ---------- Cards ---------- */
        .dcg-grid {
            display: grid;
            grid-template-columns: repeat(12, 1fr);
            gap: .75rem;
            margin-top: .5rem;
        }
        .dcg-card {
            grid-column: span 12;
            border: 1px solid var(--border);
            border-radius: 12px;
            background: var(--card);
            padding: .875rem 1rem;
        }
        @media (min-width: 720px) {
            .dcg-card.col-6 { grid-column: span 6; }
            .dcg-card.col-4 { grid-column: span 4; }
        }
        .dcg-card h4 { margin: 0 0 .25rem 0; }

        /* ---------- Accordion ---------- */
        .dcg-acc summary {
            list-style: none;
            cursor: pointer;
            padding: .75rem 1rem;
            border-bottom: 1px solid var(--border);
            font-weight: 600;
            outline-offset: 4px;
        }
        .dcg-acc summary::-webkit-details-marker { display: none; }
        .dcg-acc details[open] > summary { background: #eef2ff; }
        .dcg-acc .acc-body { padding: .75rem 1rem 1rem; }

        /* ---------- Checklist ---------- */
        .dcg-checklist label {
            display: grid;
            grid-template-columns: 28px 1fr;
            gap: .75rem;
            align-items: start;
            padding: .5rem .75rem;
            border: 1px dashed var(--border);
            border-radius: 10px;
            margin: .375rem 0;
            background: #ffffff;
        }
        .dcg-checklist input[type="checkbox"] {
            transform: translateY(3px);
            width: 20px; height: 20px;
            accent-color: var(--accent-2);
        }
        .dcg-small { font-size: .925rem; }

        /* ---------- Chips ---------- */
        .dcg-chip {
            display: inline-block;
            padding: .25rem .5rem;
            border-radius: 999px;
            background: #eef2ff;
            color: #1e40af;
            border: 1px solid #c7d2fe;
            font-size: .875rem;
            margin: .125rem .25rem .125rem 0;
        }

        /* ---------- Links ---------- */
        .dcg-content a { color: var(--link); text-underline-offset: 2px; }
        .dcg-content a:hover { text-decoration-thickness: 2px; }
    </style>

    <!-- Header -->

    <div class="dcg-wrap" role="region" aria-label="Guide body">
        <!-- TOC -->
        <nav class="dcg-toc" aria-label="On this page">
            <h3>On this page</h3>
            <a href="#intro">Introduction</a>
            <a href="#literature">Literature and best practice</a>
            <a href="#ethics">Ethical and eSafety</a>
            <a href="#implementation">Implementation challenges</a>
            <a href="#curriculum">Curriculum connections</a>
            <a href="#recommendations">Teacher recommendations</a>
        </nav>

        <!-- Content -->
        <main class="dcg-content">
            <!-- Introduction -->
            <section id="intro" class="dcg-section" aria-labelledby="intro-h">
                <div class="dcg-sec-head">
                    <h2 id="intro-h">Introduction</h2>
                </div>
                <div class="dcg-sec-body">
                    <p><strong>What is algorithmic bias?</strong><br>
                        Algorithms learn patterns from data. If the data or design overlooks diversity, the system can repeatedly produce unfair outcomes (Jonker & Rogers, 2024). Two common forms:</p>
                    <ul>
                        <li><strong>Allocative bias:</strong> unfair distribution of opportunities, for example a tool giving higher scores to some groups.</li>
                        <li><strong>Representational bias:</strong> distorted portrayals, for example image searches showing scientists as mostly male (Suresh & Guttag, 2021).</li>
                    </ul>

                    <p><strong>Why it matters in education</strong><br>
                        Schools now use AI in learning platforms, exam support, and wellbeing analytics. If biased, these systems can:</p>
                    <ul>
                        <li>Mislabel students as at risk, distort results, or restrict opportunities.</li>
                        <li>Reinforce stereotypes that affect identity, aspiration, and belonging.</li>
                        <li>Reduce trust in assessment and feedback (Baker & Hawn, 2021).</li>
                    </ul>

                    <p>In 2020 the UK’s grading algorithm downgraded many students from public schools while advantaging those from elite schools, prompting a national reversal and debate about fairness (Baker & Hawn, 2021).</p>

                    <p><strong>Teacher role:</strong> help students question how technology makes decisions, apply fairness checks, and connect this to digital citizenship and curriculum expectations on equity, ethics, and values (UNESCO, 2021).</p>
                </div>
            </section>




            <!-- Literature and Best Practice --><section id="literature" class="dcg-section" aria-labelledby="lit-h">
            <div class="dcg-sec-head">
                <h2 id="lit-h">Literature Review &amp; Synthesis</h2>
            </div>
            <div class="dcg-sec-body">

                <p><strong>Causes of Algorithmic Bias</strong></p>
                <ul>
                    <li><strong>Biased training data:</strong> AI systems inherit inequalities embedded in historical datasets, reinforcing patterns of gender, racial, or socioeconomic disadvantage (Varsha, 2023).</li>
                    <li><strong>Design and optimisation choices:</strong> When algorithms prioritise engagement or accuracy without ethical safeguards, they can amplify stereotypes or sensational content (UNESCO, 2021).</li>
                    <li><strong>Opaque models:</strong> Black box systems make it hard to see or challenge unfair reasoning, so bias can persist unnoticed.</li>
                </ul>

                <p><strong>Impacts on Education</strong></p>
                <ul>
                    <li><strong>Individual learners:</strong> Biased tools can misidentify student ability or disproportionately flag marginalised students for behavioural risk, undermining confidence and trust (Lockwood & Brown, 2025).</li>
                    <li><strong>System-level equity:</strong> AI tutoring platforms often perform better for students whose demographics match the training data, potentially widening achievement gaps (Baker & Hawn, 2021).</li>
                    <li><strong>Youth perspectives:</strong> Algorithms tuned for engagement create “filter bubbles,” shaping worldviews and limiting exposure to diverse perspectives (eSafety Commissioner, 2025).</li>
                </ul>

                <p><strong>Best practices emerging from the literature</strong></p>
                <ul>
                    <li><strong>Run fairness checks:</strong> Test classroom tools with diverse inputs, compare outcomes across groups, and document mitigation steps. Treat AI outputs as hypotheses, not verdicts (Ferrara, 2023).</li>
                    <li><strong>Teach critical questioning:</strong> Build routines where students ask what data trained a system, what is missing, who benefits, and how fairness will be tested in this task.</li>
                    <li><strong>Use diverse data and examples:</strong> Demonstrate systems with varied names, accents, images and contexts. Invite students to bring counter-examples and discuss differences observed (Shrestha & Das, 2022).</li>
                    <li><strong>Learn through inquiry:</strong> Evidence-based pedagogies include algorithm auditing by students, short case studies of real incidents, role play as “developers” tasked with debiasing a dataset, and critical media analyses that make stereotypes visible (UNSW, 2021).</li>
                </ul>

                <div class="dcg-callout good">
                    <strong>Key message</strong>
                    <p><strong>Make the abstract visible.</strong> When teachers and students test, question and document how systems behave, they reduce harm now and cultivate the critical and ethical capacities required for responsible digital citizenship.</p>
                </div>
            </div>
        </section>


            <!-- Ethical and eSafety -->
            <section id="ethics" class="dcg-section" aria-labelledby="ethics-h">
                <div class="dcg-sec-head">
                    <h2 id="ethics-h">Ethical and eSafety Considerations</h2>
                </div>
                <div class="dcg-sec-body">

                    <h3 style="color:#003366;">Ethical issues</h3>
                    <ul>
                        <li><strong>Fairness and inclusion:</strong> ensure all students are treated equitably. Make bias visible and correct it when found (UNESCO, 2021).</li>
                        <li><strong>Accountability:</strong> people make choices about data and design. Do not treat AI outputs as authority. Someone must review, explain and, if needed, reverse decisions.</li>
                        <li><strong>Transparency and explainability:</strong> black box systems limit scrutiny. Prefer tools that explain criteria or provide bias documentation (Varsha, 2023).</li>
                        <li><strong>Rights and consent:</strong> use only the data needed for learning. Seek informed consent for any sensitive data use where applicable.</li>
                    </ul>

                    <h3 style="color:#003366;">eSafety issues</h3>
                    <ul>
                        <li><strong>Content risks:</strong> recommender systems can surface harmful or extreme material. Teach students to question and report unsafe content (eSafety Commissioner, 2024).</li>
                        <li><strong>Privacy and security:</strong> protect student data. Check that tools meet Australian Privacy Principles and departmental vetting (Vic Gov, 2025).</li>
                        <li><strong>Wellbeing and identity:</strong> biased outputs and stereotypes can harm confidence and belonging. Create clear reporting and support pathways for students (eSafety Commissioner, 2025).</li>
                        <li><strong>Safety by Design:</strong> expect platforms to build in user protection, user controls and accountability. Use this language with vendors and students (eSafety Commissioner, 2024).</li>
                    </ul>

                    <div class="dcg-callout info">
                        <strong>Teacher takeaway</strong>
                        <p>Ethics and safety are not add-ons. Build fairness, accountability, transparency, privacy and wellbeing into everyday AI use to meet curriculum expectations on equity, ethics and digital citizenship (ACARA, 2025).</p>
                    </div>

                </div>
            </section>


            <!-- Implementation Challenges -->
            <section id="implementation" class="dcg-section" aria-labelledby="impl-h">
                <div class="dcg-sec-head">
                    <h2 id="impl-h">Implementation Challenges</h2>
                </div>
                <div class="dcg-sec-body">

                    <h3 style="color:#003366;">Teacher knowledge and confidence</h3>
                    <p>Many educators feel underprepared to teach about artificial intelligence. The field evolves quickly, and terms can be daunting without a computer science background. Most teachers have had little formal training in AI or data science pedagogy (UNESCO, 2021).</p>

                    <h3 style="color:#003366;">Student misconceptions and attitudes</h3>
                    <p>Learners often assume that computers are objective, a misconception known as automation bias. This can lead to over-trusting AI outputs (Goddard et al., 2012). Others may feel that AI is too complex to question, resulting in disengagement or apathy. Some students may even accept bias as “just the way it is,” reducing motivation to critique it.</p>

                    <h3 style="color:#003366;">Curriculum and resource constraints</h3>
                    <p>With crowded timetables, fitting new content can be difficult. Teachers may have limited access to age-appropriate lesson materials and, in some schools, insufficient technology to demonstrate AI concepts. The fast pace of AI developments further complicates resource relevance.</p>

                    <h3 style="color:#003366;">Student diversity and engagement</h3>
                    <p>Classes contain a wide spread of skill levels. While some students may already code, others may struggle with abstract ideas. Sensitive topics (e.g., race or gender) may also surface, requiring careful navigation and classroom management.</p>

                    <!-- Interactive Checklist -->
                    <div class="dcg-checklist" id="impl-checklist" aria-describedby="impl-check-desc">
                        <p id="impl-check-desc" class="dcg-muted dcg-small">
                            Tick each item to reflect on potential classroom challenges when teaching about AI bias.
                        </p>

                        <label>
                            <input type="checkbox" data-key="impl1">
                            <span>I feel confident in my own understanding of AI concepts and terms, or I know where to seek support if not.</span>
                        </label>

                        <label>
                            <input type="checkbox" data-key="impl2">
                            <span>I anticipate and address student misconceptions such as “computers are always objective.”</span>
                        </label>

                        <label>
                            <input type="checkbox" data-key="impl3">
                            <span>I have access to age-appropriate lesson resources and activities, or I can adapt low-tech/unplugged options.</span>
                        </label>

                        <label>
                            <input type="checkbox" data-key="impl4">
                            <span>I plan for diversity in my class, ensuring both advanced and less experienced students can engage with the topic.</span>
                        </label>
                    </div>

                </div>
            </section>

            <script>
                (function() {
                    const form = document.getElementById('impl-checklist');
                    if (!form) return;
                    const key = 'impl-checklist-v1';
                    const saved = JSON.parse(localStorage.getItem(key) || '{}');
                    form.querySelectorAll('input[type="checkbox"][data-key]').forEach(cb => {
                        if (saved[cb.dataset.key]) cb.checked = true;
                        cb.addEventListener('change', () => {
                            saved[cb.dataset.key] = cb.checked;
                            localStorage.setItem(key, JSON.stringify(saved));
                        });
                    });
                })();
            </script>




            <!-- Curriculum connections -->
            <section id="curriculum" class="dcg-section" aria-labelledby="curr-h">
                <div class="dcg-sec-head">
                    <h2 id="curr-h">Curriculum Alignment</h2>
                </div>
                <div class="dcg-sec-body">

                    <h3 style="color:#003366;">Australian Curriculum: Digital Technologies (v9.0)</h3>
                    <ul>
                        <li>Years 9 to 10 focus on ethical data practices and impacts of digital solutions. Use algorithmic bias to address: applying protocols for ethical data collection through automated processes, evaluating privacy and security, and testing algorithms for accuracy and fairness
                            <em>(AC9TDI10P02, AC9TDI10P14, AC9TDI10P06; ACARA, 2022)</em>.
                        </li>
                        <li>In Years 7 to 8, link bias to investigating data quality, acknowledging uncertainty, and considering community impacts of digital systems <em>(ACARA, 2025)</em>.</li>
                    </ul>

                    <h3 style="color:#003366;">General Capabilities</h3>
                    <ul>
                        <li><strong>Digital Literacy:</strong> investigate and evaluate digital information, detect bias and inaccuracies, protect data and make informed tool choices (ACARA, 2025).</li>
                        <li><strong>Ethical Understanding:</strong> reason about fairness, rights and responsibilities in AI use.</li>
                        <li><strong>Critical and Creative Thinking:</strong> question assumptions, test outputs, propose improvements to debias systems.</li>
                        <li><strong>Intercultural Understanding and Personal and Social Capability:</strong> consider who is represented or excluded and the wellbeing effects of biased systems.</li>
                    </ul>

                    <h3 style="color:#003366;">Other frameworks and guidance</h3>
                    <ul>
                        <li><strong>eSafety Commissioner:</strong> Safety by Design highlights service provider responsibility, user empowerment and transparency. Use it to frame tool selection and school procedures (eSafety, 2023, 2025).</li>
                        <li><strong>UNESCO:</strong> AI literacy guidance and competency frameworks emphasise inclusion, transparency and accountability in AI education (UNESCO, 2021).</li>
                    </ul>

                </div>
            </section>




        </main>
    </div>

    <script>
        // Smooth scroll and active link highlighting
        (function() {
            const tocLinks = Array.from(document.querySelectorAll('.dcg-toc a'));
            const sections = tocLinks.map(a => document.querySelector(a.getAttribute('href'))).filter(Boolean);

            const setActive = (id) => {
                tocLinks.forEach(a => a.setAttribute('aria-current', a.getAttribute('href') === id ? 'true' : 'false'));
            };

            // Smooth scroll
            tocLinks.forEach(a => {
                a.addEventListener('click', e => {
                    e.preventDefault();
                    const target = document.querySelector(a.getAttribute('href'));
                    if (!target) return;
                    window.scrollTo({ top: target.getBoundingClientRect().top + window.scrollY - 8, behavior: 'smooth' });
                    setActive(a.getAttribute('href'));
                });
            });

            // Intersection observer for active state
            const io = new IntersectionObserver((entries) => {
                entries.forEach(entry => {
                    if (entry.isIntersecting) setActive('#' + entry.target.id);
                });
            }, { rootMargin: '-40% 0px -55% 0px', threshold: 0.01 });

            sections.forEach(sec => io.observe(sec));
        })();

        // Checklist persistence
        (function() {
            const form = document.getElementById('dcg-checklist');
            if (!form) return;
            const key = 'dcg-checklist-v1';
            const saved = JSON.parse(localStorage.getItem(key) || '{}');
            form.querySelectorAll('input[type="checkbox"][data-key]').forEach(cb => {
                if (saved[cb.dataset.key]) cb.checked = true;
                cb.addEventListener('change', () => {
                    saved[cb.dataset.key] = cb.checked;
                    localStorage.setItem(key, JSON.stringify(saved));
                });
            });
        })();
    </script>
</section>
